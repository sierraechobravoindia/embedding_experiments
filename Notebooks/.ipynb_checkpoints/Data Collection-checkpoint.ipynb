{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection \n",
    "\n",
    "This notebook contains all necessary steps for data scraping and preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import PyPDF2\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import textract\n",
    "import spacy\n",
    "from stop_words import get_stop_words\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_numeric, strip_multiple_whitespaces, strip_short\n",
    "\n",
    "spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once and download sources\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once and download German language resources\n",
    "# !  python3 -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ingest raw unlabelled CRQ data to list of strings and pickle\n",
    "\n",
    "data = open(\"../Data/CRQ/Unlabelled_crs.txt\", 'r')\n",
    "\n",
    "raw_data = data.read()\n",
    "raw_data = raw_data.replace('\"\"',\"'\")\n",
    "raw_data_sep = raw_data.split('\"')\n",
    "\n",
    "raw_data_sep_nonempty = [element for element in raw_data_sep if element!=\"\\n\"]\n",
    "\n",
    "#print(raw_data_sep_nonempty[4344]) \n",
    "\n",
    "pickle.dump(raw_data_sep_nonempty, open(\"../Data/CRQ/CRQ_raw.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest labelled CRQ, convert to list of strings and pickle\n",
    "\n",
    "test_data = pd.read_pickle(\"../Data/CRQ/testset-Copy1.pkl\")\n",
    "train_data = pd.read_pickle(\"../Data/CRQ/trainset-Copy1.pkl\")\n",
    "\n",
    "CRQ_train_labelled = train_data['CR'].tolist()\n",
    "CRQ_test_labelled = test_data['CR'].tolist()\n",
    "\n",
    "#print(CRQ_train_labelled[123])\n",
    "#print(CRQ_test_labelled[123])\n",
    "\n",
    "pickle.dump(CRQ_train_labelled, open(\"../Data/CRQ/CRQ_train.pkl\",'wb'))\n",
    "pickle.dump(CRQ_test_labelled, open(\"../Data/CRQ/CRQ_test.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Books: Liefert eine Liste von Strings mit jedem Buch als ein String. \n",
    "\n",
    "path = \"/home/seb/Capstone Project/Data/Books\"\n",
    "extract =[]\n",
    "\n",
    "def book_to_string(path):\n",
    "    content=\"\"\n",
    "    file = open(path, \"rb\")\n",
    "    book = PyPDF2.PdfFileReader(file)\n",
    "    for i in range(book.getNumPages()):\n",
    "        page = book.getPage(i)\n",
    "        content += page.extractText()\n",
    "    return(content)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    extract.append(book_to_string(os.path.join(path, file)))\n",
    "pickle.dump(extract, open(\"../Data/Books_raw.pkl\",'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Patents: Liefert eine Liste von Strings mit jedem Buch als ein String. \n",
    "\n",
    "path = \"/home/seb/Capstone Project/Data/Patents\"\n",
    "extract =[]\n",
    "\n",
    "def patent_to_string(path):\n",
    "    content = textract.process(path)\n",
    "    content = content.decode(\"utf-8\", \"strict\") \n",
    "    return(content)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    extract.append(patent_to_string(os.path.join(path, file)))\n",
    "pickle.dump(extract, open(\"../Data/Patents_raw.pkl\",'wb'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure, length and sample content of raw data\n",
    "with open(\"../Data/Books_raw.pkl\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    print(len(data), type(data), type(data[0]))\n",
    "with open(\"../Data/Patents_raw.pkl\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    print(len(data), type(data), type(data[0]))\n",
    "    print(data[122])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline\n",
    "# Takes list of strings as pkl\n",
    "# Returns list of lists as pkl\n",
    "# Steps:\n",
    "# Remove Punctuation\n",
    "# Remove Numerics\n",
    "# Remove short words (short being n=1)\n",
    "# Remove multiple whitespace\n",
    "# lowercasing\n",
    "# split string into words\n",
    "# remove stopwords\n",
    "# lemmatize w/ spacy lemmatizer (takes forever, hence skipped for now)\n",
    "\n",
    "\n",
    "def preprocess(path_in, path_out):\n",
    "    \n",
    "    def split_words(string):\n",
    "        return string.split()\n",
    "        \n",
    "    def remove_stopwords(list):\n",
    "        result = [word for word in list if not word in stop_words]\n",
    "        return result\n",
    "    \n",
    "    def lemmatize(list):\n",
    "        list_lemm = []\n",
    "        for word in list:\n",
    "            doc = nlp(word)\n",
    "            for element in doc:\n",
    "                list_lemm.append(element.lemma_) \n",
    "        return list_lemm\n",
    "\n",
    "    nlp = spacy.load(\"de_core_news_sm\")\n",
    "    stop_words = get_stop_words('german')\n",
    "    \n",
    "    CRQ_data = pickle.load( open(path_in, \"rb\" ) )\n",
    "    CRQ_data = [strip_punctuation(s) for s in CRQ_data]\n",
    "    CRQ_data = [strip_numeric(s) for s in CRQ_data]\n",
    "    CRQ_data = [strip_multiple_whitespaces(s) for s in CRQ_data]\n",
    "    CRQ_data = [s.lower() for s in CRQ_data]\n",
    "    CRQ_data = [strip_short(s, minsize=3) for s in CRQ_data]\n",
    "    CRQ_data = [split_words(s) for s in CRQ_data]\n",
    "    CRQ_data = [remove_stopwords(l) for l in CRQ_data]\n",
    "    #CRQ_data = [lemmatize(element) for element in CRQ_data]\n",
    "    \n",
    "    pickle.dump(CRQ_data, open(path_out, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess unlabelled CRQ\n",
    "preprocess(\"../Data/CRQ/CRQ_1.pkl\", \"../Data/CRQ_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess labelled CRQ\n",
    "preprocess(\"../Data/CRQ/CRQ_train.pkl\", \"../Data/CRQ_train_preprocessed.pkl\")\n",
    "preprocess(\"../Data/CRQ/CRQ_test.pkl\", \"../Data/CRQ_test_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess patents\n",
    "preprocess(\"../Data/Patents_raw.pkl\", \"../Data/Patents_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess books\n",
    "preprocess(\"../Data/Books_raw.pkl\",\"../Data/Books_preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sample content of preprocessed data\n",
    "with open(\"../Data/Patents_preprocessed.pkl\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    print(data[210])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
