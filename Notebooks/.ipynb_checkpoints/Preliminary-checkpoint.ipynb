{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert raw unlabelled CRQ data to list of lists and pickle\n",
    "import pickle\n",
    "\n",
    "data = open(\"../Data/CRQ/Unlabelled_crs.txt\", 'r')\n",
    "\n",
    "raw_data = data.read()\n",
    "raw_data = raw_data.replace('\"\"',\"'\")\n",
    "raw_data_sep = raw_data.split('\"')\n",
    "raw_data_sep_nonempty = [element for element in raw_data_sep if element!=\"\\n\"]\n",
    "for i in range(500):\n",
    "    print(raw_data_sep_nonempty[i])\n",
    "    print('------------------------------------')\n",
    "\n",
    "pickle.dump( raw_data_sep_nonempty, open( \"CRQ_1.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess unlabellled CRQs \n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "CRQ_data = pickle.load( open( \"../CRQ_1.pkl\", \"rb\" ) )\n",
    "\n",
    "print(CRQ_data[4344])\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "CRQ_data_nopunc = [strip_punctuation(s) for s in CRQ_data]\n",
    "\n",
    "\n",
    "print(CRQ_data_nopunc[4344])\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "CRQ_data_nopunc_nonum = [strip_numeric(s) for s in CRQ_data_nopunc]\n",
    "print(CRQ_data_nopunc_nonum[4344])\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "\n",
    "CRQ_data_nopunc_nonum_nomultwhite = [strip_multiple_whitespaces(s) for s in CRQ_data_nopunc_nonum]\n",
    "print(CRQ_data_nopunc_nonum_nomultwhite[4344])\n",
    "\n",
    "CRQ_data_nopunc_nonum_nomultwhite_lower = [s.lower() for s in CRQ_data_nopunc_nonum_nomultwhite]\n",
    "print(CRQ_data_nopunc_nonum_nomultwhite_lower[4344])\n",
    "\n",
    "def split_words(string):\n",
    "    return string.split()\n",
    "CRQ_data_nopunc_nonum_nomultwhite_lower_split = [split_words(s) for s in CRQ_data_nopunc_nonum_nomultwhite_lower]\n",
    "\n",
    "print(CRQ_data_nopunc_nonum_nomultwhite_lower_split[4344])\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('german')\n",
    "\n",
    "def remove_stopwords(list):\n",
    "    result = [word for word in list if not word in stop_words]\n",
    "    return result\n",
    "\n",
    "CRQ_data_nopunc_nonum_nomultwhite_lower_split_nostop = [remove_stopwords(l) for l in CRQ_data_nopunc_nonum_nomultwhite_lower_split]\n",
    "\n",
    "print(CRQ_data_nopunc_nonum_nomultwhite_lower_split_nostop[4344])\n",
    "\n",
    "print(CRQ_data_nopunc_nonum_nomultwhite_lower_split_nostop[4634])\n",
    "\n",
    "pickle.dump(CRQ_data_nopunc_nonum_nomultwhite_lower_split_nostop, open(\"CR_preprocessed.pkl\",'w'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Unlabelled CRQs and save them as raw elements to disk\n",
    "\n",
    "data = open(\"Unlabelled_crs.txt\", 'r')\n",
    "\n",
    "raw_data = data.read()\n",
    "\n",
    "raw_data = raw_data.replace('\"\"',\"'\")\n",
    "\n",
    "raw_data_sep = raw_data.split('\"')\n",
    "\n",
    "raw_data_sep_nonempty = [element for element in raw_data_sep if element!=\"\\n\"]\n",
    "\n",
    "for i in range(500):\n",
    "    print(raw_data_sep_nonempty[i])\n",
    "    print('------------------------------------')\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump( raw_data_sep_nonempty, open( \"CRQ_1.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Books: Liefert eine Liste von Strings mit jedem Buch als ein String. \n",
    "\n",
    "path = \"/home/seb/Capstone Project/Data/Books_2\"\n",
    "path_specific = \"/home/seb/Desktop/Book Collection/Thermodynamik Der Verbrennungskraftmaschine - Rudolf Pischinger.pdf\"\n",
    "\n",
    "extract =[]\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    extract.append(book_to_string(os.path.join(path, file)))\n",
    "pickle.dump(extract, open(../Data/Books/Books_raw.pkl,'wb'))\n",
    "    \n",
    "def book_to_string(path):\n",
    "    content=\"\"\n",
    "    file = open(path, \"rb\")\n",
    "    book = PyPDF2.PdfFileReader(file)\n",
    "    for i in range(book.getNumPages()):\n",
    "        page = book.getPage(i)\n",
    "        page_content += page.extractText()\n",
    "    return(content)\n",
    "\n",
    "\n",
    "def test(path, path_specific):\n",
    "    pdf_file = open(path_specific, \"rb\")\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    print(number_of_pages)\n",
    "    for i in range(0, read_pdf.getNumPages()):\n",
    "        page = read_pdf.getPage(i)\n",
    "        print(page.extractText())\n",
    "\n",
    "    page = read_pdf.getPage(0)\n",
    "    page_content = page.extractText()\n",
    "    print(page_content)\n",
    "\n",
    "def page_count(path):\n",
    "    cnt = 0\n",
    "    sum = 0\n",
    "    for file in os.listdir(path):\n",
    "        pdf_file = \n",
    "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "        number_of_pages = read_pdf.getNumPages()\n",
    "        print(number_of_pages)\n",
    "        sum+=number_of_pages\n",
    "        cnt+=1\n",
    "    return sum, cnt\n",
    "\n",
    "def word_count_2(path):\n",
    "    cnt = 0\n",
    "    sum_pages = 0\n",
    "    sum_words = 0\n",
    "    for file in os.listdir(path):\n",
    "        sum_words_book = 0\n",
    "        pdf_file = open(os.path.join(path, file), \"rb\")\n",
    "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "        number_of_pages = read_pdf.getNumPages()\n",
    "        \n",
    "        for i in range(0, number_of_pages):\n",
    "            page = read_pdf.getPage(i)\n",
    "            page_content = page.extractText()\n",
    "            num_words_page = len(page_content.split())\n",
    "            sum_words_book+=num_words_page    \n",
    "        sum_pages+=number_of_pages\n",
    "        sum_words +=sum_words_book\n",
    "        cnt+=1\n",
    "    return sum_pages, sum_words, cnt\n",
    "\n",
    "def word_count(path):\n",
    "    cnt = 0\n",
    "    sum = 0\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".pdf\"):  \n",
    "            print(file)\n",
    "            text = textract.process(os.path.join(path, file))\n",
    "            print(len(text))\n",
    "            res = len(text.split())\n",
    "            cnt+=1\n",
    "            sum+=res\n",
    "    print(cnt)\n",
    "    print(res)\n",
    "    \n",
    "# print(page_count(path))\n",
    "#print(word_count_2(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding sollte sein iso-8859-1\n",
    "#Patent Imports\n",
    "#Liefert eine Liste von Listen mit den Patenten\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import textract\n",
    "\n",
    "path_patents = \"/home/seb/Capstone Project/Data/Patents_2\"\n",
    "path_specific = \"/home/seb/Capstone Project/Patents2/DE112006000494B4.pdf\"\n",
    "\n",
    "def test(path, path_specific):\n",
    "    pdf_file = open(path_specific, \"rb\")\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    print(number_of_pages)\n",
    "    for i in range(0, read_pdf.getNumPages()):\n",
    "        page = read_pdf.getPage(i)\n",
    "        print(page.extractText())\n",
    "\n",
    "    page = read_pdf.getPage(0)\n",
    "    page_content = page.extractText()\n",
    "    print(page_content)\n",
    "\n",
    "def page_count(path):\n",
    "    cnt = 0\n",
    "    sum = 0\n",
    "    for file in os.listdir(path):\n",
    "        pdf_file = open(os.path.join(path, file), \"rb\")\n",
    "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "        number_of_pages = read_pdf.getNumPages()\n",
    "        print(number_of_pages)\n",
    "        sum+=number_of_pages\n",
    "        cnt+=1\n",
    "    return sum, cnt\n",
    "\n",
    "def word_count_2(path):\n",
    "    cnt = 0\n",
    "    sum_pages = 0\n",
    "    sum_words = 0\n",
    "    for file in os.listdir(path):\n",
    "        sum_words_book = 0\n",
    "        pdf_file = open(os.path.join(path, file), \"rb\")\n",
    "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "        number_of_pages = read_pdf.getNumPages()\n",
    "        \n",
    "        for i in range(0, number_of_pages):\n",
    "            page = read_pdf.getPage(i)\n",
    "            page_content = page.extractText()\n",
    "            num_words_page = len(page_content.split())\n",
    "            sum_words_book+=num_words_page    \n",
    "        sum_pages+=number_of_pages\n",
    "        sum_words +=sum_words_book\n",
    "        cnt+=1\n",
    "    return sum_pages, sum_words, cnt\n",
    "\n",
    "def word_count(path):\n",
    "    cnt = 0\n",
    "    sum = 0\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".pdf\"):  \n",
    "            # print(file)\n",
    "            text = textract.process(os.path.join(path, file))\n",
    "            # print(text)\n",
    "            res = len(text.split())\n",
    "            print(res)\n",
    "            cnt+=1\n",
    "            sum+=res\n",
    "    return sum, cnt\n",
    "\n",
    "def patents_to_sequence(path):\n",
    "    pat_cnt = 0\n",
    "    data = []\n",
    "    for file in os.listdir(path):\n",
    "        text=textract.process(os.path.join(path, file), language='ger')\n",
    "        data.append(text)\n",
    "        pat_cnt+=1\n",
    "    return data, pat_cnt\n",
    "\n",
    "# test(path, path_specific) \n",
    "# print(page_count(path))\n",
    "# print(word_count(path))\n",
    "\n",
    "\n",
    "#text = textract.process('path/to/norwegian.pdf', method='tesseract',language='nor',)\n",
    "\n",
    "raw_data, number = patents_to_sequence(path_patents)\n",
    "print(raw_data[:3])\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liefert eine Liste von Strings mit jedem Patent als eine Wortliste.\n",
    "\n",
    "path_books = \"/home/seb/Capstone Project/Data/Patents_2\"\n",
    "\n",
    "def books_to_seq_2(path):\n",
    "    sum_books = 0\n",
    "    for file in os.listdir(path):\n",
    "        text = textract.process(os.path.join(path, file))\n",
    "        sum_books+=1\n",
    "    return text, sum_books \n",
    "\n",
    "def books_to_sequence(path):\n",
    "    raw_data = []\n",
    "    sum_pages = 0\n",
    "    sum_books = 0\n",
    "    for file in os.listdir(path):\n",
    "        pdf_file = open(os.path.join(path, file), \"rb\")\n",
    "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "        number_of_pages = read_pdf.getNumPages()\n",
    "        sum_pages+=number_of_pages\n",
    "        sum_books+=1\n",
    "        for i in range(number_of_pages):\n",
    "            page = read_pdf.getPage(i)\n",
    "            page_content = page.extractText() \n",
    "            raw_data.append(page_content)\n",
    "    return raw_data, sum_pages, sum_books\n",
    "\n",
    "data, pages, books = books_to_sequence(path_books)\n",
    "#data, books = books_to_seq2(path_books)\n",
    "print(books)\n",
    "\n",
    "print(data[:10000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
