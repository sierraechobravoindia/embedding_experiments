{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "This notebook contains 4 of ther 5 metrics used to evaluate and compare the different embeddings.\n",
    "\n",
    "## M1 Downstream Classification\n",
    "\n",
    "This metric can be found in the notebook \"LSTM.ipynb\" as it turned out to be a micro-project in itself :-)\n",
    "\n",
    "## M2 Clustering by Subsystem\n",
    "\n",
    "TThe basic idea of this metric is clustering / classification in the embedding vector space.\n",
    "To implement, sklearn KNN classification with cosine distance as a distance measure is used. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html\n",
    "\n",
    "\n",
    "## M3 Word Analogy\n",
    "\n",
    "Here, word analogies in the form \n",
    "\n",
    "a:b::c:d\n",
    "\n",
    "e.g. King:Man::Queen:Woman\n",
    "\n",
    "or in vector addition: King-Man+Woman=Queen \n",
    "\n",
    "are employed as a metric. The basic idea is to count how many word analogies from a predefined list are found in the different embeddings.\n",
    "\n",
    "\n",
    "## M4 Word Similarity List\n",
    "\n",
    "This is a well-known intrinsic measure for word embeddings. I adapt it here to the domain specific realm of automotive engineering. For this purpose I compiled a list of wordpairs like the well known \"wordsim353\" and asked colleagues to provide a similarity measure between 0 and 10. This human similarity score is then (Pearson-) correlated to the cosine-similarity of the embeddings.\n",
    "\n",
    "\n",
    "## M5 Visualizations\n",
    "\n",
    "Visualizations are not a quantitative metric, however they provide (with limitations) qualitative insights about the properties of edmbeddings. Here I employ t-SNE, one of the most commomly used algorithms for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import fasttext\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Vectors\n",
    "\n",
    "#Load pretrained FastText\n",
    "model_ft =  KeyedVectors.load_word2vec_format('../WordEmbeddings/FastText/cc.de.300.vec') \n",
    "\n",
    "#Load domain specific FastText\n",
    "model_ds_ft = FastText.load(\"../WordEmbeddings/ft_ds.model\")\n",
    "\n",
    "#Load domain specific word2vec\n",
    "model_ds_w2v = Word2Vec.load(\"../WordEmbeddings/w2v_ds.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_ds_w2v.wv.vocab))\n",
    "print(len(model_ds_ft.wv.vocab))\n",
    "print(len(model_ft.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ft['Getriebe']\n",
    "model_ft.get_vector('Zylinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "word2vec_CRs = gensim.models.Word2Vec.load(\"../RelatedWork/w2v_CRs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_matrix = word2vec_CRs.wv.get_keras_embedding(train_embeddings=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "model = gensim.models.Word2Vec.load(\"../RelatedWork/w2v_CRs\")\n",
    "similarities = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1: Downstram Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 LSTM\n",
    "# Hier einfach die Pipeline übernehmen und die verschiedenen embeddings einsetzen. \n",
    "# Dazu brauche ich dann word vector und token liste... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2: Subsystem Classification in Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering oder Klassifikation. Mal schauen...\n",
    "\n",
    "+ Suche je 10 Worte aus 3 Gruppen aus (später vielleicht 30 Worte aus 5 Gruppen...)\n",
    "+ Schreibe die Worte in ein Array\n",
    "+ Mache Lookup des Word Vectors für diese Worte mit der jeweiligen Repräsentation und speichere in einem Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fingerübungen zu KNN Clustering und KNN Classification\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# just using the cosine distance function\n",
    "x=[[1,2,3]]\n",
    "y=[[4,5,6]]\n",
    "print(cosine_distances(x,y))\n",
    "\n",
    "\n",
    "#KNN Clustering w/ cosine distance\n",
    "samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
    "knn = NearestNeighbors(metric='cosine')\n",
    "knn.fit(samples)\n",
    "\n",
    "\n",
    "# KNN Classification\n",
    "X_train =[[1], [2], [3], [4],[5]]\n",
    "y_train = [0,0,1,1,1]\n",
    "knnc=KNeighborsClassifier(metric='cosine', n_neighbors=3)\n",
    "knnc.fit(X_train, y_train)\n",
    "y_pred=knnc.predict(X_train) \n",
    "print(y_pred)\n",
    "print(accuracy_score(y_train, y_pred, normalize=True, sample_weight=None))                            \n",
    "print(knnc.predict([[1.4]]))\n",
    "print(knnc.predict_proba([[0.9]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M2 Clustering by subsystem\n",
    "\n",
    "engine = ['motor', 'einspritzung', 'zündung', \"zündkerze\", \"wasserpumpe\", \"nockenwelle\", \"kurbelwelle\", \"starter\", \"generator\"]\n",
    "transmission = ['getriebe', \"gang\", \"schlupf\", \"kupplung\", \"einlegen\", \"zahnrad\", \"gang\", \"automatik\", \"schaltgetriebe\"]\n",
    "control_unit = [\"steuergerät\", \"funktion\", \"applikation\", \"regler\", \"tsk\", \"architektur\", \"variable\", \"parameter\"]\n",
    "vehicle = [\"can\", \"kombiinstrument\", \"gaspedal\", \"fahrwerk\", \"chassis\", \"pedal\", \"mil\", \"esp\", \"abs\", \"lenkrad\"]\n",
    "exhaust = [\"lambdasonde\", \"dpf\", \"opf\", \"katalysator\", \"abgas\", \"emission\", \"nox\"]\n",
    "\n",
    "classes = [engine, transmission, control_unit, vehicle, exhaust]\n",
    "models = [model_ds_ft, model_ds_w2v, model_ft]\n",
    "data = engine+transmission+control_unit+vehicle+exhaust\n",
    "\n",
    "X_train = [model_ds_ft[word] for word in data]\n",
    "y_train = [1]*len(engine)+[2]*len(transmission)+[3]*len(control_unit)+[4]*len(vehicle)+[5]*len(exhaust)\n",
    "\n",
    "knnc=KNeighborsClassifier(metric='cosine', n_neighbors=3)\n",
    "knnc.fit(X_train, y_train)\n",
    "y_pred=knnc.predict(X_train) \n",
    "\n",
    "print(accuracy_score(y_train, y_pred, normalize=True, sample_weight=None))\n",
    "\n",
    "def check_existence(list, model):\n",
    "    flag = True\n",
    "    for word in list:\n",
    "        if word not in model.wv.vocab:\n",
    "            print(\"{} in Class {} not found in Embedding {}\".format(word, c, model))\n",
    "            flag = False\n",
    "    return flag\n",
    "\n",
    "for model in models:\n",
    "    for c in classes:\n",
    "        #print(check_existence(c, model))\n",
    "        pass\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred, labels=None, sample_weight=None)\n",
    "    \n",
    "print(cm)\n",
    "plt.imshow(cm, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3: Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M3 Word Analogy\n",
    "\n",
    "#Word Analogies:\n",
    "#(positive=['woman', 'king'], negative=['man'])\n",
    "\n",
    "a1 = [\"diesel\", \"dieselmotor\", \"benzin\", \"ottomotor\",]\n",
    "a2 = [\"motor\", \"motorsteuergerät\", \"getriebe\", \"getriebesteuergerät\"]\n",
    "a3 = [\"benzin\",\"zündkerze\",\"diesel\",\"selbstzündung\"]\n",
    "a4 = [\"wasser\", \"wasserpumpe\", \"benzin\", \"benzinpumpe\"]\n",
    "a5 = [\"benzin\", \"tank\", \"strom\", \"batterie\"]\n",
    "a6 = [\"can\",\"botschaft\",\"flexray\",\"pdu\"]\n",
    "\n",
    "analogies = [a1,a2,a3,a4,a5,a6]\n",
    "\n",
    "#pretty print for results: print(\"{}: {:.4f}\".format(*result[0]))\n",
    "\n",
    "def analogy(quartet):\n",
    "    #result_ft = model_ft.most_similar(positive=[quartet[0], quartet[1]], negative=[quartet[2]])\n",
    "    result_ds_ft = model_ds_ft.most_similar(positive=[quartet[0], quartet[1]], negative=[quartet[2]])\n",
    "    result_ds_w2v = model_ds_w2v.most_similar(positive=[quartet[0], quartet[1]], negative=[quartet[2]])\n",
    "\n",
    "    return result_ds_w2v\n",
    "for a in analogies:\n",
    "    print(analogy(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M4 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"list\"\n",
    "\n",
    "#Loading data from disk\n",
    "if DATA_SOURCE == \"disk\":\n",
    "    data = pd.read_csv('../Data/sim_list_N_o.tsv', delimiter ='\\t',header = None)\n",
    "    df_human_score = pd.read_csv('../Data/human_score.tsv', header = None) \n",
    "    e1 = [e.lower() for e in data[0]]\n",
    "    e2 = [e.lower() for e in data[1]]\n",
    "    words = [(e1,e2) for (e1,e2) in zip(e1,e2)]\n",
    "    human_score = [s for s in df_human_score[0]]\n",
    "    \n",
    "    \n",
    "#Loading data from list\n",
    "if DATA_SOURCE == \"list\":\n",
    "    words_original=[(\"riemenstartergenerator\",\"rsg\"),(\"motorsteuergerät\",\"msg\"),(\"haltemanagementsystem\",\"hms\"),\n",
    "       (\"batterymanagementsystem\",\"bms\"),(\"stateofcharge\",\"soc\"),(\"konzernschaltprogramm\",\"ksp\"),\n",
    "        (\"hochvoltkoordinator\",\"hvk\"),(\"geschwindigkeitsregelanlage\",\"gra\"),(\"triebstrangkoordinator\",\"tsk\"),\n",
    "        (\"functionondemand\",\"fod\"),(\"automatictransmissionfluid\",\"atf\"),(\"on boarddiagnose\",\"obd\"),(\"dieselpartikelfilter\",\"dpf\"),\n",
    "        (\"ottopartikelfilter\",\"opf\"),(\"electronicstabilitycontrol\",\"esc\"),(\"antriebssteuergeraet\",\"asg\"),(\"vorderachse\",\"va\"),\n",
    "        (\"intelligenterparkassistent\",\"ipa\"),(\"praediktivereffizienzassistent\",\"pea\"),(\"intelligentspeedassistent\",\"isa\"),\n",
    "        (\"Motor\",\"Zylinder\"),(\"tsk\",\"laengsbeschleunigungsregler\"),(\"Drehzahl\",\"Schub\"),(\"Hybrid\",\"phev\"),(\"Brennstoffzelle\",\"Anode\"),\n",
    "        (\"Brennstoffzelle\",\"Kathode\"),(\"kraftstoff\",\"diesel\"),(\"getriebe\",\"k0\"),(\"Display\",\"Anzeige\"),(\"Monitoring\",\"Überwachung\"),\n",
    "        (\"Wasserstoff\",\"H2\"),(\"motor\",\"vkm\"),(\"EMaschine\",\"Strom\"),(\"Bremspedal\",\"Kriechen\"),(\"Emissionen\",\"nox\"),(\"Druckventil\",\"entllueftung\"),\n",
    "       (\"fahrpedal\",\"moment\"),(\"wählhebel\",\"einlegen\"),(\"kupplung\",\"getriebe\"),(\"dl382\",\"schaltung\"),(\"phev\",\"cbev\"),(\"pumpe\",\"öl\"),\n",
    "        (\"wärme\",\"heizen\"),(\"saugrohrdruck\",\"drosseln\"),(\"drehzahl\",\"anzeige\"),(\"drehzahl\",\"antrieb\"),(\"leerlauf\",\"schaltung\"),\n",
    "    (\"mqb\",\"golf\"),(\"meb\",\"bev\"),(\"entprellung\",\"signal\"),(\"brennstoffzelle\",\"purgeventil\"),(\"adpcus\",\"signal\"),\n",
    "    (\"startstopp\",\"leerlauf\"),(\"drehmoment\",\"achsfunktionalität\"),(\"rampe\",\"momentengradient\"),(\"rekuperation\",\"potential\"),\n",
    "    (\"asg\",\"msg\"),(\"antrieb\",\"leistung\"),(\"sekundaerachse\",\"ankoppeln\"),(\"waehlhebel\",\"kupplung\")]\n",
    "    \n",
    "    words=[(\"motorsteuergerät\",\"msg\"),\n",
    "       (\"motor\",\"zylinder\"),\n",
    "       (\"drehzahl\",\"schub\"),\n",
    "       (\"brennstoffzelle\",\"anode\"),\n",
    "       (\"kraftstoff\",\"diesel\"),\n",
    "       (\"display\",\"anzeige\"),\n",
    "       (\"monitoring\",\"überwachung\"),\n",
    "       (\"emissionen\",\"nox\"),\n",
    "       (\"kupplung\",\"getriebe\"),\n",
    "       (\"pumpe\",\"oel\"),\n",
    "       (\"wärme\",\"heizen\"),\n",
    "       (\"drehzahl\",\"anzeige\"),\n",
    "       (\"drehzahl\",\"antrieb\"),\n",
    "       (\"leerlauf\",\"schaltung\"),\n",
    "       (\"asg\",\"msg\"),\n",
    "       (\"antrieb\",\"leistung\")]\n",
    "    human_score = (9, 6, 3, 6, 7, 8, 8, 7, 6, 5, 7, 4, 5, 1, 5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_ds_ft = [model_ds_ft.wv.similarity(e1, e2) for (e1, e2) in words]\n",
    "similarity_ds_w2v = [model_ds_w2v.wv.similarity(e1, e2) for (e1, e2) in words]\n",
    "similarity_ft = [model_ft.wv.similarity(e1, e2) for (e1, e2) in words]\n",
    "\n",
    "score_ds_ft = pearsonr(similarity_ds_ft, human_score)\n",
    "score_ds_w2v = pearsonr(similarity_ds_w2v, human_score)\n",
    "score_ft = pearsonr(similarity_ft, human_score)\n",
    "\n",
    "print(\"Score with domain specific FastText: {}\".format(score_ds_ft[0]))\n",
    "print(\"Score with domain specific word2vec: {}\".format(score_ds_w2v[0]))\n",
    "print(\"Score with general FastText: {}\".format(score_ft[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plots a t_SNE of the complete embedding. Might give an impression of the quality of learned embeddings? \n",
    "\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label some random data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model_ds_ft)\n",
    "plot_with_matplotlib(x_vals, y_vals, labels)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_ds_ft\n",
    "\n",
    "keys1 = ['benzin', 'diesel']\n",
    "keys2 =['motor','getriebe']\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys2:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in model.most_similar(word, topn=20):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=12, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=12)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "title = \"t_SNE: 20 neighbours of '{}' and '{}' with domain specific FastText\".format(keys2[0], keys2[1], model)\n",
    "    \n",
    "tsne_plot_similar_words(title, keys, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
