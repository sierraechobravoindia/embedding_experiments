{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metriken\n",
    "## M1 Downstram Classification Task\n",
    "Hier werde ich ein einfaches LSTM aufbauen und verwenden. Die gelernten word vectors werden im embedding layer eingesetzt und dann wird die accuracy als Metrik verwendet.\n",
    "## M2 Clustering by Subsystem\n",
    "+ compile a list of 3-5 subsystems and collect 10-20 words that belong to the subsystem\n",
    "+ build a clustering algorithm in the embedding space based on the cosine-distance\n",
    "\n",
    "Hier mal schauen, ob es einen sklearn -Algo gibt, der ein clustering mit einer beliebigen Metrik macht.\n",
    "\n",
    "Cosine Distance is available as function in sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html\n",
    "\n",
    "\n",
    "Alternative wäre es, die Qualität mit einem Silhuette Score auf den bekannten Clustern zu machen: Also gar kein Clustering, sondern die Wahrheit als Basis für die Zuordnung zu Clustern nehmen und dann mit dem Silhuette Score  \n",
    "Hier wäre es vielleicht sinnvoll, den Score von einer Matrix aus Zufallszahlen zu vergleichen? \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\n",
    "\n",
    "\n",
    "Ach ja, umrechnung von distance in similarity nicht vergessen: Wenn das eine zwischen 0 und 1 läuft, beispielsweise einfach similarity = 1-distance ...\n",
    "\n",
    "Vielversprechender Kandidat ist hierarchial clustering:\n",
    "\n",
    "https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/\n",
    "\n",
    "Hier kann man bei sklearn direkt cosine-similarity verwenden:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "\n",
    "#Vectorizing\n",
    "X = CountVectorizer().fit_transform(docs)\n",
    "X = TfidfTransformer().fit_transform(X)\n",
    "#Clustering\n",
    "X = X.todense()\n",
    "threshold = 0.1\n",
    "Z = hierarchy.linkage(X,\"average\", metric=\"cosine\")\n",
    "C = hierarchy.fcluster(Z, threshold, criterion=\"distance\")\n",
    "\n",
    "## M3 Word Analogy\n",
    "\n",
    "a:b::c:d\n",
    "\n",
    "da solte eigentlich gensim eine Funktion bieten, mit der man das direkt machen kann\n",
    "\n",
    "\n",
    "\n",
    "## M4 Word Similarity List\n",
    "Hier werde ich Norberts Liste verwenden und dann die cosine-Distanz der word embeddings mit der gemittelten Similarity korrelieren (PEARSON correlation oder sowas? )\n",
    "\n",
    "Hier hat auch gensim eine Funktion, alternativ einfach einen Pearson-Koeffizienten nehmen...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import fasttext\n",
    "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Vectors\n",
    "\n",
    "#Load pretrained FastText\n",
    "model_ft =  KeyedVectors.load_word2vec_format('../WordEmbeddings/FastText/cc.de.300.vec') \n",
    "\n",
    "#Load domain specific FastText\n",
    "model_ds_ft = FastText.load(\"../WordEmbeddings/ft_ds.model\")\n",
    "\n",
    "#Load domain specific word2vec\n",
    "model_ds_w2v = Word2Vec.load(\"../WordEmbeddings/w2v_ds.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0302, -0.0021,  0.0121, -0.0206, -0.0801, -0.0096,  0.0264,\n",
       "       -0.0125, -0.0196,  0.0497,  0.0041, -0.0541,  0.0067,  0.0442,\n",
       "        0.0008, -0.0069,  0.0596,  0.017 ,  0.0262, -0.0511, -0.0142,\n",
       "        0.0484,  0.0196, -0.0175,  0.0066,  0.0532, -0.0125, -0.0182,\n",
       "        0.0032, -0.0496, -0.0075,  0.0318, -0.0316,  0.0004, -0.0266,\n",
       "        0.0096,  0.0427, -0.0186,  0.029 , -0.013 ,  0.0304, -0.0372,\n",
       "       -0.0102,  0.0125,  0.0337,  0.0347,  0.0449, -0.0143, -0.0444,\n",
       "       -0.0149, -0.0488,  0.0004, -0.0041,  0.0387, -0.0062, -0.0812,\n",
       "       -0.013 , -0.0464,  0.0023, -0.0503, -0.024 ,  0.0208,  0.0545,\n",
       "        0.0016,  0.0231,  0.0503,  0.0433, -0.0195, -0.0071, -0.0187,\n",
       "        0.0035, -0.0352, -0.0085, -0.0411,  0.0492,  0.0076,  0.0315,\n",
       "       -0.015 , -0.0301,  0.0106, -0.075 ,  0.0019, -0.0759,  0.019 ,\n",
       "        0.006 ,  0.022 , -0.0294, -0.023 ,  0.0019, -0.0538,  0.0117,\n",
       "        0.03  ,  0.0147,  0.0253, -0.0059, -0.0207,  0.0409,  0.0044,\n",
       "       -0.004 ,  0.0769, -0.0253,  0.072 , -0.0412, -0.0126,  0.0351,\n",
       "       -0.0096, -0.0202,  0.0042, -0.0523, -0.046 ,  0.0217,  0.0083,\n",
       "       -0.0052, -0.0331, -0.0168,  0.0096,  0.0007,  0.0263,  0.0126,\n",
       "       -0.044 , -0.0544, -0.0203,  0.0561, -0.037 , -0.0677,  0.0131,\n",
       "       -0.0117,  0.0306, -0.0059, -0.0707, -0.0355,  0.1027, -0.0084,\n",
       "       -0.0221, -0.075 , -0.0186,  0.0243,  0.0352, -0.0015, -0.0657,\n",
       "        0.0002, -0.0123,  0.0836, -0.0053, -0.0294,  0.0301,  0.0118,\n",
       "        0.0235,  0.0036, -0.0885, -0.0235,  0.0543, -0.0255, -0.0158,\n",
       "        0.0333,  0.1035, -0.0265,  0.0018,  0.0473, -0.0639, -0.0328,\n",
       "        0.0135, -0.0445, -0.0366,  0.0047,  0.0061, -0.0508, -0.077 ,\n",
       "       -0.0303,  0.0062,  0.0052,  0.0209,  0.0461,  0.0094, -0.0346,\n",
       "        0.0284, -0.007 ,  0.0301,  0.0371,  0.0254,  0.0515, -0.0177,\n",
       "        0.0009, -0.0283,  0.0037, -0.0053,  0.0275,  0.0256,  0.009 ,\n",
       "       -0.0639,  0.0465, -0.0523, -0.0668, -0.0325,  0.015 ,  0.0206,\n",
       "        0.0133,  0.0586,  0.078 , -0.0004, -0.0024, -0.0209, -0.0107,\n",
       "        0.0418,  0.0444,  0.001 , -0.0279,  0.0534, -0.0347, -0.0003,\n",
       "       -0.0208,  0.0482, -0.0173,  0.0296,  0.0027,  0.0439, -0.0305,\n",
       "        0.0222,  0.0002,  0.0379, -0.0113,  0.0214,  0.008 ,  0.0012,\n",
       "        0.0314, -0.0063, -0.0343, -0.0039, -0.0349, -0.0002,  0.0403,\n",
       "        0.0123, -0.013 , -0.0005, -0.0143, -0.0175,  0.048 , -0.0152,\n",
       "        0.0383,  0.0123,  0.0323,  0.0398,  0.0175,  0.0003,  0.0342,\n",
       "        0.0677,  0.0089, -0.0571, -0.0226, -0.0295, -0.0313,  0.0531,\n",
       "        0.0524, -0.0303, -0.0111, -0.0035, -0.0361, -0.0573,  0.0244,\n",
       "       -0.0043,  0.0205,  0.0173,  0.0301, -0.062 ,  0.0735,  0.0561,\n",
       "       -0.0267,  0.0015,  0.0513, -0.0265, -0.0211,  0.0406, -0.0659,\n",
       "        0.0345, -0.0102,  0.0181,  0.0582,  0.0231, -0.0319, -0.0338,\n",
       "       -0.0515, -0.0131, -0.022 , -0.0227, -0.0375, -0.0108,  0.1147,\n",
       "        0.0361,  0.0256,  0.0034, -0.003 , -0.0179, -0.0692, -0.0035,\n",
       "       -0.0249,  0.0145, -0.0058,  0.003 ,  0.0007,  0.055 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_ft['Getriebe']\n",
    "model_ft.get_vector('Zylinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "word2vec_CRs = gensim.models.Word2Vec.load(\"../RelatedWork/w2v_CRs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_matrix = word2vec_CRs.wv.get_keras_embedding(train_embeddings=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.5336101992975493, 0.46638980070245073), SpearmanrResult(correlation=0.39999999999999997, pvalue=0.6), 98.86685552407933)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "model = gensim.models.Word2Vec.load(\"../RelatedWork/w2v_CRs\")\n",
    "similarities = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1: Downstram Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 LSTM\n",
    "# Hier einfach die Pipeline übernehmen und die verschiedenen embeddings einsetzen. \n",
    "# Dazu brauche ich dann word vector und token liste... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2: Subsystem Classification in Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering oder Klassifikation. Mal schauen...\n",
    "\n",
    "+ Suche je 10 Worte aus 3 Gruppen aus (später vielleicht 30 Worte aus 5 Gruppen...)\n",
    "+ Schreibe die Worte in ein Array\n",
    "+ Mache Lookup des Word Vectors für diese Worte mit der jeweiligen Repräsentation und speichere in einem Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02536815]]\n",
      "[0 0 0 0 0]\n",
      "0.4\n",
      "[0]\n",
      "[[0.66666667 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "# Fingerübungen zu KNN Clustering und KNN Classification\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# just using the cosine distance function\n",
    "x=[[1,2,3]]\n",
    "y=[[4,5,6]]\n",
    "print(cosine_distances(x,y))\n",
    "\n",
    "\n",
    "#KNN Clustering w/ cosine distance\n",
    "samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
    "knn = NearestNeighbors(metric='cosine')\n",
    "knn.fit(samples)\n",
    "\n",
    "\n",
    "# KNN Classification\n",
    "X_train =[[1], [2], [3], [4],[5]]\n",
    "y_train = [0,0,1,1,1]\n",
    "knnc=KNeighborsClassifier(metric='cosine', n_neighbors=3)\n",
    "knnc.fit(X_train, y_train)\n",
    "y_pred=knnc.predict(X_train) \n",
    "print(y_pred)\n",
    "print(accuracy_score(y_train, y_pred, normalize=True, sample_weight=None))                            \n",
    "print(knnc.predict([[1.4]]))\n",
    "print(knnc.predict_proba([[0.9]])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 1 1 2 2 2 2 3 3 3 3 2]\n",
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Metrics using KNN-Classification mit cosine-distance\n",
    "# Idea: Use Training accuracy as a measure for the embedding.\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "family = ['father', 'mother', 'bike', 'daughter', 'sister']\n",
    "vehicle = ['engine', 'transmission', 'chassis', 'gasoline']\n",
    "nature = ['lion', 'tree', 'mountain', 'river', 'ignition']\n",
    "\n",
    "data = family+vehicle+nature\n",
    "\n",
    "X_train = [model[word] for word in data]\n",
    "y_train = [1,1,1,1,1,2,2,2,2,3,3,3,3,3]\n",
    "\n",
    "knnc=KNeighborsClassifier(metric='cosine', n_neighbors=3)\n",
    "knnc.fit(X_train, y_train)\n",
    "y_pred=knnc.predict(X_train) \n",
    "print(y_pred)\n",
    "print(accuracy_score(y_train, y_pred, normalize=True, sample_weight=None))\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M2 Clustering by subsystem\n",
    "\n",
    "engine = ['motor', 'einspritzung', 'zündung']\n",
    "transmission = ['getriebe', \"gang\", \"wählhebel\", \"fahrstufe\", \"schlupf\", \"kupplung\"]\n",
    "control_unit = [\"funktion\", \"applikation\", \"regler\"]\n",
    "vehicle = [\"can\", \"kombiinstrument\", 'flexray']\n",
    "exhaust = []\n",
    "\n",
    "\n",
    "#dann wie oben...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3: Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('motorsteuerung', 0.7153784036636353), ('empfangszeit', 0.6869455575942993), ('abfühleinrichtungen', 0.6833028197288513), ('steuermodul', 0.6775274872779846), ('überwachen', 0.6653618812561035), ('bzs', 0.6628608107566833), ('hochvolt', 0.6606370210647583), ('fahrerinformationen', 0.6585701704025269), ('motorsteuer', 0.6548113822937012), ('empd', 0.6495625972747803)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# M3 Word Analogy\n",
    "result = model_ds_w2v.most_similar(positive=['motor', 'motorsteuergerät'], negative=['getriebe'])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M4 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9922858194799438, 9.997264988456583e-06)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M4 Word similarity\n",
    "\n",
    "\n",
    "# using the Builtin function would be cool, however some glitch\n",
    "# from gensim.test.utils import datapath\n",
    "# /usr/local/lib/python3.6/dist-packages/gensim/test/test_data/wordsim353.tsv\n",
    "# similarities = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "\n",
    "# using Pearson manually\n",
    "\n",
    "\n",
    "\n",
    "x = [-3,-2,-1,0,1,2,3]\n",
    "y = [-1.5,-1,-0.5,0,0.5,1,2]\n",
    "pearsonr(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/sim_list_N_o.tsv', delimiter ='\\t',header = None)\n",
    "human_scores = pd.read_csv('../Data/human_score.tsv', header = None) \n",
    "data.head()\n",
    "\n",
    "e1 = [e.lower() for e in data[0]]\n",
    "e2 = [e.lower() for e in data[1]]\n",
    "\n",
    "word_pairs = zip(e1,e2)\n",
    "\n",
    "similarity_ds_ft = [model_ds_ft.wv.similarity(e1,e2) for (e1,e2) in word_pairs]\n",
    "similarity_ds_w2v = [model_ds_w2v.wv.similarity(e1,e2) for (e1,e2) in word_pairs]\n",
    "similarity_ft = [model_ft.wv.similarity(e1,e2) for (e1,e2) in word_pairs]\n",
    "similarity_human = \n",
    "\n",
    "score_ft = pearsonr()\n",
    "score_ds_ft = pearsonr()\n",
    "score_ds_w2v = pearsonr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
